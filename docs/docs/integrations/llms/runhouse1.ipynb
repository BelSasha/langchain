{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9597802c",
   "metadata": {},
   "source": [
    "# Runhouse\n",
    "\n",
    "The [Runhouse](https://github.com/run-house/runhouse) allows remote compute and data across environments and users. See the [Runhouse docs](https://www.run.house/docs).\n",
    "\n",
    "This example goes over how to use LangChain and [Runhouse](https://github.com/run-house/runhouse) to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.\n",
    "\n",
    "**Note**: Code uses `SelfHosted` name instead of the `Runhouse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6066fede-2300-4173-9722-6f01f4fa34b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain/libs/langchain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain/libs/langchain\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.21 (from langchain==0.1.9)\n",
      "  Obtaining dependency information for langchain-community<0.1,>=0.0.21 from https://files.pythonhosted.org/packages/8d/cc/387b93205020d23151c039e73805062c749a452a417fc578c7ea69efd469/langchain_community-0.0.27-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.26 (from langchain==0.1.9)\n",
      "  Obtaining dependency information for langchain-core<0.2,>=0.1.26 from https://files.pythonhosted.org/packages/53/b3/ae022560a8b104525b4ac1a97a557e3aa05dd0d233bb5284f7c63509742f/langchain_core-0.1.30-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain==0.1.9)\n",
      "  Obtaining dependency information for langsmith<0.2.0,>=0.1.0 from https://files.pythonhosted.org/packages/ec/cf/dccbfcf35f33dcb356b0168a851342339c7e79cec5a2e6f4e045fc3aee53/langsmith-0.1.22-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.1.22-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (1.26.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain==0.1.9) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.9) (3.21.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.9) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.9) (2.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain==0.1.9) (4.2.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain==0.1.9) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain==0.1.9) (3.9.15)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.9) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.9) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.9) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.9) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.9) (2023.7.22)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain==0.1.9) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain==0.1.9) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sashabelousovrh/miniforge3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.9) (1.0.0)\n",
      "Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.22-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: langchain\n",
      "  Building editable for langchain (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langchain: filename=langchain-0.1.9-py3-none-any.whl size=6064 sha256=ceb80f0f74659bd9b9c6d668663ae244e94a425b604281f1aa105aedd426dad1\n",
      "  Stored in directory: /private/var/folders/rj/chszqnp129g1jnbt6g8_0snw0000gn/T/pip-ephem-wheel-cache-4und5p55/wheels/22/c5/c8/e6b753eeddd3506f02c883cf293b3c128a99a2a2ce1819718f\n",
      "Successfully built langchain\n",
      "Installing collected packages: langsmith, langchain-core, langchain-community, langchain\n",
      "Successfully installed langchain-0.1.9 langchain-community-0.0.27 langchain-core-0.1.30 langsmith-0.1.22\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain/docs/docs/integrations/llms\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet runhouse\n",
    "%cd /Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain/libs/langchain\n",
    "%pip install -e . \n",
    "%cd /Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain/docs/docs/integrations/llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd735de5-0eb2-4fb6-adcf-8776eabeddf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/sashabelousovrh/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import runhouse as rh\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import SelfHostedHuggingFaceLLM, SelfHostedPipeline\n",
    "from langchain_community.llms.self_hosted_hugging_face import _generate_text, _load_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d6866e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-07 13:59:21.835063 | Saving config for rh-a10x-ssh-secret to Den\n",
      "INFO | 2024-03-07 13:59:21.995021 | Saving secrets for rh-a10x-ssh-secret to Vault\n"
     ]
    }
   ],
   "source": [
    "# For an on-demand A10G with the cheapest provider (default)\n",
    "gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"g5.2xlarge\", use_spot=False)\n",
    "\n",
    "# For an on-demand A10G with AWS\n",
    "# gpu = rh.ondemand_cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')\n",
    "\n",
    "# For an existing cluster\n",
    "# gpu = rh.cluster(ips=['<ip of the cluster>'],\n",
    "#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n",
    "#                  name='rh-a10x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4f2c8d-a7f0-46a3-a276-889fcb9f5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env = rh.env(reqs=[\"transformers\", \"torch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035dea0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29421ebc-3d96-43c7-a31e-2d8f9283157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-07 13:59:41.887909 | Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "INFO | 2024-03-07 13:59:42.735377 | Authentication (publickey) successful!\n",
      "WARNING | 2024-03-07 13:59:43.448415 | Server was started with Runhouse version (0.0.19), but local Runhouse version is (0.0.20)\n",
      "INFO | 2024-03-07 13:59:43.452130 | Server rh-a10x is up.\n",
      "INFO | 2024-03-07 13:59:43.464212 | Copying package from file:///Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain to: rh-a10x\n",
      "INFO | 2024-03-07 13:59:47.810518 | Calling base_env.install\n",
      "INFO | 2024-03-07 13:59:49.114660 | Time to call base_env.install: 1.3 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-07 13:59:52.457303 | Sending module _load_transformer to rh-a10x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_transformer_remote = rh.function(fn=_load_transformer).to(gpu, env=model_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052008ef-3792-48fe-90bb-cac049efb6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-07 14:00:01.271664 | Copying package from file:///Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain to: rh-a10x\n",
      "INFO | 2024-03-07 14:00:03.362922 | Calling base_env.install\n",
      "INFO | 2024-03-07 14:00:04.655512 | Time to call base_env.install: 1.29 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-07 14:00:08.103214 | Sending module _generate_text to rh-a10x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text_remote = rh.function(_generate_text).to(gpu, env=model_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3458d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "SelfHostedPipeline relies on the pickle module. You will need to set allow_dangerous_deserialization=True if you want to opt-in to allow deserialization of data using pickle.Data can be compromised by a malicious actor if not handled properly to include a malicious payload that when deserialized with pickle can execute arbitrary code. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mSelfHostedHuggingFaceLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_load_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_transformer_remote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_text_remote\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(gpu, env\u001b[38;5;241m=\u001b[39mmodel_env)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_community/llms/self_hosted_hugging_face.py:190\u001b[0m, in \u001b[0;36mSelfHostedHuggingFaceLLM.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct the pipeline remotely using an auxiliary function.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03mThe load function needs to be importable to be imported\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03mand run on the server, i.e. in a module and not a REPL or closure.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mThen, initialize the remote inference function.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m load_fn_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_MODEL_ID),\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_TASK),\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    189\u001b[0m }\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mload_fn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_fn_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_community/llms/self_hosted.py:158\u001b[0m, in \u001b[0;36mSelfHostedPipeline.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Init the pipeline with an auxiliary function.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03mThe load function must be in global scope to be imported\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03mand run on the server, i.e. in a module and not a REPL or closure.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03mThen, initialize the remote inference function.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_dangerous_deserialization\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelfHostedPipeline relies on the pickle module. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou will need to set allow_dangerous_deserialization=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif you want to opt-in to allow deserialization of data using pickle.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData can be compromised by a malicious actor if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot handled properly to include \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma malicious payload that when deserialized with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickle can execute arbitrary code. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: SelfHostedPipeline relies on the pickle module. You will need to set allow_dangerous_deserialization=True if you want to opt-in to allow deserialization of data using pickle.Data can be compromised by a malicious actor if not handled properly to include a malicious payload that when deserialized with pickle can execute arbitrary code. "
     ]
    }
   ],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(name=\"gemma-2b-it\", model_id=\"gemma-2b-it\", model_load_fn=load_transformer_remote, inference_fn=generate_text_remote).to(gpu, env=model_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88709cd",
   "metadata": {},
   "source": [
    "You can also load more custom models through the SelfHostedHuggingFaceLLM interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22820c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(\n",
    "    model_id=\"google/flan-t5-small\",\n",
    "    task=\"text2text-generation\",\n",
    "    hardware=gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c3746",
   "metadata": {},
   "source": [
    "Using a custom load function, we can load a custom pipeline directly on the remote hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline():\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        pipeline,\n",
    "    )\n",
    "\n",
    "    model_id = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def inference_fn(pipeline, prompt, stop=None):\n",
    "    return pipeline(prompt)[0][\"generated_text\"][len(prompt) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d50dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(\n",
    "    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Who is the current US president?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08575f",
   "metadata": {},
   "source": [
    "You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23023b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = load_pipeline()\n",
    "llm = SelfHostedPipeline.from_pipeline(\n",
    "    pipeline=pipeline, hardware=gpu, model_reqs=[\"pip:./\", \"transformers\", \"torch\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb447a1",
   "metadata": {},
   "source": [
    "Instead, we can also send it to the hardware's filesystem, which will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "rh.blob(pickle.dumps(pipeline), path=\"models/pipeline.pkl\").save().to(\n",
    "    gpu, path=\"models\"\n",
    ")\n",
    "\n",
    "llm = SelfHostedPipeline.from_pipeline(pipeline=\"models/pipeline.pkl\", hardware=gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
