{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9597802c",
   "metadata": {},
   "source": [
    "# Runhouse\n",
    "\n",
    "The [Runhouse](https://github.com/run-house/runhouse) allows remote compute and data across environments and users. See the [Runhouse docs](https://runhouse-docs.readthedocs-hosted.com/en/latest/).\n",
    "\n",
    "This example goes over how to use LangChain and [Runhouse](https://github.com/run-house/runhouse) to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.\n",
    "\n",
    "**Note**: Code uses `SelfHosted` name instead of the `Runhouse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6066fede-2300-4173-9722-6f01f4fa34b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet \"runhouse[sky]\"\n",
    "%pip install --upgrade --quiet 'skypilot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb585dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import runhouse as rh\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import SelfHostedHuggingFaceLLM, SelfHostedPipeline\n",
    "from langchain_community.llms.self_hosted_hugging_face import _generate_text, _load_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d6866e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-10 13:41:48.427922 | Saving config for rh-a10x-ssh-secret to Den\n",
      "INFO | 2024-03-10 13:41:48.586415 | Saving secrets for rh-a10x-ssh-secret to Vault\n"
     ]
    }
   ],
   "source": [
    "# For an on-demand A100 with GCP, Azure, or Lambda\n",
    "gpu = rh.cluster(name=\"rh-a10x\", instance_type=\"A100:1\", use_spot=False)\n",
    "\n",
    "# For an on-demand A10G with AWS (no single A100s on AWS)\n",
    "# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')\n",
    "\n",
    "# For an existing cluster\n",
    "# gpu = rh.cluster(ips=['<ip of the cluster>'],\n",
    "#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n",
    "#                  name='rh-a10x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035dea0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b7dcb5-7a74-4513-9ad6-aee1b4193b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env = rh.env(reqs=[\"transformers\", \"torch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07884665-9684-4342-8eb1-4a9c517755b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-10 13:41:59.535978 | Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "INFO | 2024-03-10 13:42:00.383861 | Authentication (publickey) successful!\n",
      "2024-03-10 15:42:00,388| ERROR   | Problem setting SSH Forwarder up: Couldn't open tunnel :32300 <> 127.0.0.1:32300 might be in use or destination not reachable\n",
      "ERROR | 2024-03-10 13:42:00.388296 | Problem setting SSH Forwarder up: Couldn't open tunnel :32300 <> 127.0.0.1:32300 might be in use or destination not reachable\n",
      "INFO | 2024-03-10 13:42:01.120154 | Connected (version 2.0, client OpenSSH_8.2p1)\n",
      "INFO | 2024-03-10 13:42:01.952478 | Authentication (publickey) successful!\n",
      "WARNING | 2024-03-10 13:42:02.675400 | Server was started with Runhouse version (0.0.19), but local Runhouse version is (0.0.20)\n",
      "INFO | 2024-03-10 13:42:02.678318 | Server rh-a10x is up.\n",
      "INFO | 2024-03-10 13:42:02.685062 | Copying package from file:///Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain to: rh-a10x\n",
      "INFO | 2024-03-10 13:42:04.563031 | Calling base_env.install\n",
      "INFO | 2024-03-10 13:42:05.864888 | Time to call base_env.install: 1.3 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-10 13:42:09.530630 | Sending module _load_transformer to rh-a10x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_transformer_remote = rh.function(fn=_load_transformer).to(gpu, env=model_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197d13d2-3925-461a-9634-4ca795622fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-10 13:42:14.461141 | Copying package from file:///Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain to: rh-a10x\n",
      "INFO | 2024-03-10 13:42:16.262320 | Calling base_env.install\n",
      "INFO | 2024-03-10 13:42:17.572399 | Time to call base_env.install: 1.31 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-10 13:42:21.472036 | Sending module _generate_text to rh-a10x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text_remote = rh.function(_generate_text).to(gpu, env=model_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1562d356-dde0-4d8d-8abf-0701487ccc5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"SelfHostedHuggingFaceLLM\" object has no field \"_name\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m SelfHostedHuggingFaceLLM_Remote \u001b[38;5;241m=\u001b[39m \u001b[43mrh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSelfHostedHuggingFaceLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(system\u001b[38;5;241m=\u001b[39mgpu, env\u001b[38;5;241m=\u001b[39mmodel_env)\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/resources/module.py:1222\u001b[0m, in \u001b[0;36mmodule\u001b[0;34m(cls, name, system, env, dryrun)\u001b[0m\n\u001b[1;32m   1217\u001b[0m name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1218\u001b[0m     cls_pointers[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m cls_pointers \u001b[38;5;28;01melse\u001b[39;00m _generate_default_name(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1219\u001b[0m )\n\u001b[1;32m   1221\u001b[0m module_subclass \u001b[38;5;241m=\u001b[39m _module_subclass_factory(\u001b[38;5;28mcls\u001b[39m, cls_pointers)\n\u001b[0;32m-> 1222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule_subclass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module_init_only\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdryrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdryrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpointers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_pointers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/resources/module.py:1075\u001b[0m, in \u001b[0;36m_module_subclass_factory.<locals>._module_init_only\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_module_init_only\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1074\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m-> 1075\u001b[0m     \u001b[43mModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/resources/module.py:62\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, pointers, signature, endpoint, name, system, env, dryrun, provenance, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     49\u001b[0m     pointers: Optional[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     58\u001b[0m ):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    Runhouse Module object\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdryrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdryrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovenance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovenance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system \u001b[38;5;241m=\u001b[39m _get_cluster_from(\n\u001b[1;32m     64\u001b[0m         system \u001b[38;5;129;01mor\u001b[39;00m _current_cluster(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m), dryrun\u001b[38;5;241m=\u001b[39mdryrun\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env \u001b[38;5;241m=\u001b[39m env\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/resources/resource.py:59\u001b[0m, in \u001b[0;36mResource.__init__\u001b[0;34m(self, name, dryrun, provenance, access_level, visibility, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     37\u001b[0m ):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Runhouse abstraction for objects that can be saved, shared, and reused.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m        - Secret :py:class:`.secret.Secret`\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rns_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# TODO validate that name complies with a simple regex\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/builtins/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/pydantic/main.py:357\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__setattr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \"SelfHostedHuggingFaceLLM\" object has no field \"_name\""
     ]
    }
   ],
   "source": [
    "SelfHostedHuggingFaceLLM_Remote = rh.module(cls=SelfHostedHuggingFaceLLM).to(system=gpu, env=model_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3458d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"SelfHostedHuggingFaceLLM\" object has no field \"_name\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mSelfHostedHuggingFaceLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_load_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_transformer_remote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_text_remote\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(gpu, env\u001b[38;5;241m=\u001b[39mmodel_env)\n",
      "File \u001b[0;32m~/PycharmProjects/LangchainIntegration/langchain/libs/community/langchain_community/llms/self_hosted_hugging_face.py:190\u001b[0m, in \u001b[0;36mSelfHostedHuggingFaceLLM.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct the pipeline remotely using an auxiliary function.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03mThe load function needs to be importable to be imported\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03mand run on the server, i.e. in a module and not a REPL or closure.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mThen, initialize the remote inference function.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m load_fn_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_TASK),\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;66;03m#TODO: set to auto according to the api or remove\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    189\u001b[0m }\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mload_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/LangchainIntegration/langchain/libs/community/langchain_community/llms/self_hosted.py:125\u001b[0m, in \u001b[0;36mSelfHostedPipeline.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Init the pipeline with an auxiliary function.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    The load function must be in global scope to be imported\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    and run on the server, i.e. in a module and not a REPL or closure.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    Then, initialize the remote inference function.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28msuper\u001b[39m(LLM, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    128\u001b[0m     remote_load_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_load_fn\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/resources/resource.py:59\u001b[0m, in \u001b[0;36mResource.__init__\u001b[0;34m(self, name, dryrun, provenance, access_level, visibility, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     31\u001b[0m     name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     37\u001b[0m ):\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Runhouse abstraction for objects that can be saved, shared, and reused.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m        - Secret :py:class:`.secret.Secret`\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rns_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# TODO validate that name complies with a simple regex\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/builtins/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/pydantic/main.py:357\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__setattr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \"SelfHostedHuggingFaceLLM\" object has no field \"_name\""
     ]
    }
   ],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(name=\"gemma-2b-it\", model_id=\"gemma-2b-it\", model_load_fn=load_transformer_remote, inference_fn=generate_text_remote).to(gpu, env=model_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88709cd",
   "metadata": {},
   "source": [
    "You can also load more custom models through the SelfHostedHuggingFaceLLM interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22820c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(\n",
    "    model_id=\"google/flan-t5-small\",\n",
    "    task=\"text2text-generation\",\n",
    "    hardware=gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c3746",
   "metadata": {},
   "source": [
    "Using a custom load function, we can load a custom pipeline directly on the remote hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline():\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        pipeline,\n",
    "    )\n",
    "\n",
    "    model_id = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def inference_fn(pipeline, prompt, stop=None):\n",
    "    return pipeline(prompt)[0][\"generated_text\"][len(prompt) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d50dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(\n",
    "    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Who is the current US president?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08575f",
   "metadata": {},
   "source": [
    "You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23023b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = load_pipeline()\n",
    "llm = SelfHostedPipeline.from_pipeline(\n",
    "    pipeline=pipeline, hardware=gpu, model_reqs=[\"pip:./\", \"transformers\", \"torch\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb447a1",
   "metadata": {},
   "source": [
    "Instead, we can also send it to the hardware's filesystem, which will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "rh.blob(pickle.dumps(pipeline), path=\"models/pipeline.pkl\").save().to(\n",
    "    gpu, path=\"models\"\n",
    ")\n",
    "\n",
    "llm = SelfHostedPipeline.from_pipeline(pipeline=\"models/pipeline.pkl\", hardware=gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
