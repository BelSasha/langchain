{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9597802c",
   "metadata": {},
   "source": [
    "# Runhouse\n",
    "\n",
    "The [Runhouse](https://github.com/run-house/runhouse) allows remote compute and data across environments and users. See the [Runhouse docs](https://runhouse-docs.readthedocs-hosted.com/en/latest/).\n",
    "\n",
    "This example goes over how to use LangChain and [Runhouse](https://github.com/run-house/runhouse) to interact with models hosted on your own GPU, or on-demand GPUs on AWS, GCP, AWS, or Lambda.\n",
    "\n",
    "**Note**: Code uses `SelfHosted` name instead of the `Runhouse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6066fede-2300-4173-9722-6f01f4fa34b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anthropic 0.3.11 requires anyio<4,>=3.5.0, but you have anyio 4.3.0 which is incompatible.\n",
      "langchain 0.1.12 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.1.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "types-requests 2.31.0.20240125 requires urllib3>=2, but you have urllib3 1.26.18 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet runhouse\n",
    "%pip install --upgrade --quiet \"skypilot[aws]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb585dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import runhouse as rh\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import SelfHostedHuggingFaceLLM, SelfHostedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d6866e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-20 09:37:14.735394 | Saving config for sasha-rh-a10x-ssh-secret to Den\n",
      "INFO | 2024-03-20 09:37:14.898213 | Saving secrets for sasha-rh-a10x-ssh-secret to Vault\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 809.1/809.1 kB 23.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.14)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.6/71.6 kB 13.1 MB/s eta 0:00:00\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Collecting langchain-core<0.2.0,>=0.1.31\n",
      "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 260.9/260.9 kB 30.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.28\n",
      "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 21.3 MB/s eta 0:00:00\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 83.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 9.5 MB/s eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 9.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (4.3.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.5/138.5 kB 22.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 616.0/616.0 kB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, packaging, orjson, mypy-extensions, jsonpointer, greenlet, typing-inspect, SQLAlchemy, marshmallow, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed SQLAlchemy-2.0.28 dataclasses-json-0.6.4 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 tenacity-8.2.3 typing-inspect-0.9.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  'Collecting langchain\\n  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 809.1/809.1 kB 23.2 MB/s eta 0:00:00\\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.14)\\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.3)\\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\\nCollecting langsmith<0.2.0,>=0.1.17\\n  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.6/71.6 kB 13.1 MB/s eta 0:00:00\\nCollecting dataclasses-json<0.7,>=0.5.7\\n  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\\nCollecting tenacity<9.0.0,>=8.1.0\\n  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\\nCollecting jsonpatch<2.0,>=1.33\\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\\nCollecting langchain-core<0.2.0,>=0.1.31\\n  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 260.9/260.9 kB 30.8 MB/s eta 0:00:00\\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\\nCollecting langchain-text-splitters<0.1,>=0.0.1\\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\\nCollecting langchain-community<0.1,>=0.0.28\\n  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 21.3 MB/s eta 0:00:00\\nCollecting SQLAlchemy<3,>=1.4\\n  Downloading SQLAlchemy-2.0.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 83.9 MB/s eta 0:00:00\\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.28.2)\\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\\nCollecting marshmallow<4.0.0,>=3.18.0\\n  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.4/49.4 kB 9.5 MB/s eta 0:00:00\\nCollecting typing-inspect<1,>=0.4.0\\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\\nCollecting jsonpointer>=1.9\\n  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\\nCollecting packaging<24.0,>=23.2\\n  Downloading packaging-23.2-py3-none-any.whl (53 kB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 9.8 MB/s eta 0:00:00\\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (4.3.0)\\nCollecting orjson<4.0.0,>=3.9.14\\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.5/138.5 kB 22.7 MB/s eta 0:00:00\\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.1)\\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\\nCollecting greenlet!=0.4.17\\n  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 616.0/616.0 kB 6.3 MB/s eta 0:00:00\\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\\nCollecting mypy-extensions>=0.3.0\\n  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\\nInstalling collected packages: tenacity, packaging, orjson, mypy-extensions, jsonpointer, greenlet, typing-inspect, SQLAlchemy, marshmallow, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-text-splitters, langchain-community, langchain\\n  Attempting uninstall: packaging\\n    Found existing installation: packaging 24.0\\n    Uninstalling packaging-24.0:\\n      Successfully uninstalled packaging-24.0\\nSuccessfully installed SQLAlchemy-2.0.28 dataclasses-json-0.6.4 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 tenacity-8.2.3 typing-inspect-0.9.0\\n',\n",
       "  '')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For an on-demand A100 with GCP, Azure, or Lambda\n",
    "gpu = rh.cluster(name='sasha-rh-a10x', instance_type='g5.4xlarge', provider='aws', region='eu-central-1')\n",
    "gpu.run(commands=[\"pip install langchain\"])\n",
    "\n",
    "# For an on-demand A10G with AWS (no single A100s on AWS)\n",
    "# gpu = rh.cluster(name='rh-a10x', instance_type='g5.2xlarge', provider='aws')\n",
    "\n",
    "# For an existing cluster\n",
    "# gpu = rh.cluster(ips=['<ip of the cluster>'],\n",
    "#                  ssh_creds={'ssh_user': '...', 'ssh_private_key':'<path_to_key>'},\n",
    "#                  name='rh-a10x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b7dcb5-7a74-4513-9ad6-aee1b4193b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_env = rh.env(\n",
    "    name=\"model_env15\",\n",
    "    reqs=[\"transformers\", \"torch\", \"accelerate\", \"huggingface-hub\"],\n",
    "    secrets=[\"huggingface\"]  # need for downloading google/gemma-2b-it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0228abaf-0eb3-4828-a153-640d60790ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-20 09:37:41.416454 | SSH tunnel on to server's port 32300 via server's ssh port 22 already created with the cluster.\n",
      "INFO | 2024-03-20 09:37:41.675983 | Server sasha-rh-a10x is up.\n",
      "INFO | 2024-03-20 09:37:41.681327 | Copying package from file:///Users/sashabelousovrh/PycharmProjects/LangchainIntegration/langchain to: sasha-rh-a10x\n",
      "INFO | 2024-03-20 09:37:43.965711 | Calling huggingface._write_to_file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mSecrets already exist in ~/.cache/huggingface/token.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-20 09:37:45.115689 | Time to call huggingface._write_to_file: 1.15 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-20 09:37:50.178477 | Calling model_env15.install\n",
      "INFO | 2024-03-20 09:37:51.321045 | Time to call model_env15.install: 1.14 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-20 09:37:56.039099 | Sending module LangchainLLMModelPipeline to sasha-rh-a10x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-03-20 09:38:05.927212 | Calling LangchainLLMModelPipeline._remote_init\n",
      "INFO | 2024-03-20 09:38:07.083693 | Time to call LangchainLLMModelPipeline._remote_init: 1.16 seconds\n",
      "INFO | 2024-03-20 09:38:07.089691 | Calling LangchainLLMModelPipeline.load_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mgemma-2b-it is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "\u001b[0m\u001b[36mIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "\u001b[0m\u001b[36mTraceback (most recent call last):\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "\u001b[0m\u001b[36m    response.raise_for_status()\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "\u001b[0m\u001b[36m    raise HTTPError(http_error_msg, response=self)\n",
      "\u001b[0m\u001b[36mrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/gemma-2b-it/resolve/main/tokenizer_config.json\n",
      "\u001b[0m\u001b[36m\n",
      "\u001b[0m\u001b[36mThe above exception was the direct cause of the following exception:\n",
      "\u001b[0m\u001b[36m\n",
      "\u001b[0m\u001b[36mTraceback (most recent call last):\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n",
      "\u001b[0m\u001b[36m    resolved_file = hf_hub_download(\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "\u001b[0m\u001b[36m    return fn(*args, **kwargs)\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1403, in hf_hub_download\n",
      "\u001b[0m\u001b[36m    raise head_call_error\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1261, in hf_hub_download\n",
      "\u001b[0m\u001b[36m    metadata = get_hf_file_metadata(\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "\u001b[0m\u001b[36m    return fn(*args, **kwargs)\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1667, in get_hf_file_metadata\n",
      "\u001b[0m\u001b[36m    r = _request_wrapper(\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 385, in _request_wrapper\n",
      "\u001b[0m\u001b[36m    response = _request_wrapper(\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 409, in _request_wrapper\n",
      "\u001b[0m\u001b[36m    hf_raise_for_status(response)\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 352, in hf_raise_for_status\n",
      "\u001b[0m\u001b[36m    raise RepositoryNotFoundError(message, response) from e\n",
      "\u001b[0m\u001b[36mhuggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-65faae89-2af776967419a78772760c1e;67d8fac9-fcf7-43e7-85ca-87acdc7b043f)\n",
      "\u001b[0m\u001b[36m\n",
      "\u001b[0m\u001b[36mRepository Not Found for url: https://huggingface.co/gemma-2b-it/resolve/main/tokenizer_config.json.\n",
      "\u001b[0m\u001b[36mPlease make sure you specified the correct `repo_id` and `repo_type`.\n",
      "\u001b[0m\u001b[36mIf you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "\u001b[0m\u001b[36m\n",
      "\u001b[0m\u001b[36mThe above exception was the direct cause of the following exception:\n",
      "\u001b[0m\u001b[36m\n",
      "\u001b[0m\u001b[36mTraceback (most recent call last):\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/runhouse/servers/env_servlet.py\", line 39, in wrapper\n",
      "\u001b[0m\u001b[36m    output = func(*args, **kwargs)\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/runhouse/servers/env_servlet.py\", line 118, in call_local\n",
      "\u001b[0m\u001b[36m    return obj_store.call_local(\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/runhouse/servers/obj_store.py\", line 964, in call_local\n",
      "\u001b[0m\u001b[36m    res = method(*args, **kwargs)\n",
      "\u001b[0m\u001b[36m  File \"/home/ubuntu/langchain/libs/community/langchain_community/llms/self_hosted_hugging_face.py\", line 37, in load_model\n",
      "\u001b[0m\u001b[36m    self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, torch_dtype=torch.float16,\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 767, in from_pretrained\n",
      "\u001b[0m\u001b[36m    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 600, in get_tokenizer_config\n",
      "\u001b[0m\u001b[36m    resolved_config_file = cached_file(\n",
      "\u001b[0m\u001b[36m  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "\u001b[0m\u001b[36m    raise EnvironmentError(\n",
      "\u001b[0m\u001b[36mOSError: gemma-2b-it is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "\u001b[0m\u001b[36mIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR | 2024-03-20 09:38:18.259120 | \u001b[36mError calling load_model on LangchainLLMModelPipeline on server: gemma-2b-it is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\u001b[0m\n",
      "ERROR | 2024-03-20 09:38:18.262832 | \u001b[36mTraceback: Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/gemma-2b-it/resolve/main/tokenizer_config.json\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1403, in hf_hub_download\n",
      "    raise head_call_error\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1261, in hf_hub_download\n",
      "    metadata = get_hf_file_metadata(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1667, in get_hf_file_metadata\n",
      "    r = _request_wrapper(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 385, in _request_wrapper\n",
      "    response = _request_wrapper(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 409, in _request_wrapper\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 352, in hf_raise_for_status\n",
      "    raise RepositoryNotFoundError(message, response) from e\n",
      "huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-65faae89-2af776967419a78772760c1e;67d8fac9-fcf7-43e7-85ca-87acdc7b043f)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/gemma-2b-it/resolve/main/tokenizer_config.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/runhouse/servers/env_servlet.py\", line 39, in wrapper\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/runhouse/servers/env_servlet.py\", line 118, in call_local\n",
      "    return obj_store.call_local(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/runhouse/servers/obj_store.py\", line 964, in call_local\n",
      "    res = method(*args, **kwargs)\n",
      "  File \"/home/ubuntu/langchain/libs/community/langchain_community/llms/self_hosted_hugging_face.py\", line 37, in load_model\n",
      "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, torch_dtype=torch.float16,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 767, in from_pretrained\n",
      "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 600, in get_tokenizer_config\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: gemma-2b-it is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "gemma-2b-it is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m self_hosted_llm \u001b[38;5;241m=\u001b[39m \u001b[43mSelfHostedHuggingFaceLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemma-2b-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhardware\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_env\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/LangchainIntegration/langchain/libs/community/langchain_community/llms/self_hosted_hugging_face.py:185\u001b[0m, in \u001b[0;36mSelfHostedHuggingFaceLLM.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m LangchainLLMModelPipeline_remote_instance \u001b[38;5;241m=\u001b[39m LangchainLLMModelPipeline_remote(model_id\u001b[38;5;241m=\u001b[39mmodel_id, task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[1;32m    184\u001b[0m _load_fn_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_fn_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_ref \u001b[38;5;241m=\u001b[39m \u001b[43mLangchainLLMModelPipeline_remote_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_load_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.pipline is ok\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m LangchainLLMModelPipeline_remote\u001b[38;5;241m.\u001b[39minterface_fn\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/resources/module.py:479\u001b[0m, in \u001b[0;36mModule.__getattribute__.<locals>.RemoteMethodWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# stream_logs and run_name are both supported args here, but we can't include them explicitly because\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# the local code path here will throw an error if they are included and not supported in the\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# method signature.\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremote\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/servers/http/http_client.py:302\u001b[0m, in \u001b[0;36mHTTPClient.call\u001b[0;34m(self, key, method_name, data, serialization, resource_address, run_name, stream_logs, remote, run_async, save)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    290\u001b[0m     key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    300\u001b[0m ):\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"wrapper to temporarily support cluster's call signature\"\"\"\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_address\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_async\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/servers/http/http_client.py:380\u001b[0m, in \u001b[0;36mHTTPClient.call_module_method\u001b[0;34m(self, key, method_name, data, serialization, resource_address, run_name, stream_logs, remote, run_async, save, system)\u001b[0m\n\u001b[1;32m    378\u001b[0m resp \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(responses_json)\n\u001b[1;32m    379\u001b[0m output_type \u001b[38;5;241m=\u001b[39m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 380\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_formatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_formatter\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_type \u001b[38;5;241m==\u001b[39m OutputType\u001b[38;5;241m.\u001b[39mCONFIG:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# If this was a `.remote` call, we don't need to recreate the system and connection, which can be\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# slow, we can just set it explicitly.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    387\u001b[0m         system\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m system\u001b[38;5;241m.\u001b[39mrns_address \u001b[38;5;241m==\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    390\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniforge3/envs/rh_langchain_env/lib/python3.9/site-packages/runhouse/servers/http/http_utils.py:241\u001b[0m, in \u001b[0;36mhandle_response\u001b[0;34m(response_data, output_type, err_str, log_formatter)\u001b[0m\n\u001b[1;32m    239\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn_exception\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mreset_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mTraceback: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn_traceback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mreset_color\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m fn_exception\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m output_type \u001b[38;5;241m==\u001b[39m OutputType\u001b[38;5;241m.\u001b[39mSTDOUT:\n\u001b[1;32m    243\u001b[0m     res \u001b[38;5;241m=\u001b[39m response_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mOSError\u001b[0m: gemma-2b-it is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(model_id=\"google/gemma-2b-it\", hardware=gpu, env=model_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035dea0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b4f06a-c698-4e69-9fee-f2efbe35976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88709cd",
   "metadata": {},
   "source": [
    "You can also load more custom models through the SelfHostedHuggingFaceLLM interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22820c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(\n",
    "    model_id=\"google/flan-t5-small\",\n",
    "    task=\"text2text-generation\",\n",
    "    hardware=gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"What is the capital of Germany?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c3746",
   "metadata": {},
   "source": [
    "Using a custom load function, we can load a custom pipeline directly on the remote hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893eb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pipeline():\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        pipeline,\n",
    "    )\n",
    "\n",
    "    model_id = \"gpt2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def inference_fn(pipeline, prompt, stop=None):\n",
    "    return pipeline(prompt)[0][\"generated_text\"][len(prompt) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d50dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = SelfHostedHuggingFaceLLM(\n",
    "    model_load_fn=load_pipeline, hardware=gpu, inference_fn=inference_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Who is the current US president?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08575f",
   "metadata": {},
   "source": [
    "You can send your pipeline directly over the wire to your model, but this will only work for small models (<2 Gb), and will be pretty slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23023b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = load_pipeline()\n",
    "llm = SelfHostedPipeline.from_pipeline(\n",
    "    pipeline=pipeline, hardware=gpu, model_reqs=[\"pip:./\", \"transformers\", \"torch\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb447a1",
   "metadata": {},
   "source": [
    "Instead, we can also send it to the hardware's filesystem, which will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "rh.blob(pickle.dumps(pipeline), path=\"models/pipeline.pkl\").save().to(\n",
    "    gpu, path=\"models\"\n",
    ")\n",
    "\n",
    "llm = SelfHostedPipeline.from_pipeline(pipeline=\"models/pipeline.pkl\", hardware=gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
